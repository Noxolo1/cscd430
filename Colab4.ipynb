{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# Colab 4: Stream Processing using Spark (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will learn how to process stream data using Spark's [Structured Streaming API](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)."
      ],
      "metadata": {
        "id": "l19Q-weVdMFi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbYZoVVWOZA5"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cells below!\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhzk3GE6S9RC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fdb597-4d59-44ca-8aa3-a54a3afaa120"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u392-ga-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_aXQDamh3p"
      },
      "source": [
        "#Import libraries\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiQXwAPF-A8V",
        "outputId": "5e07d9a3-aac8-4985-dc59-f7f2c1a48013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO_IcxgquzhI"
      },
      "source": [
        "Structured Streaming supports several [input sources](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources) for reading in a streaming fashion.\n",
        "\n",
        "Please create a new folder \"activity-data\" under your current working folder (i.e., \"content\" folder). Download the zip file from Canvas, extract all the files, and then upload these files to \"activity-data\" folder.\n",
        "\n",
        "Let’s first read in the **static** version of one file in the dataset as a DataFrame. We are going to work with the Heterogeneity Human Activity Recognition Dataset. The data consists of smartphone and smartwatch sensor readings from a variety of devices—specifically, the accelerometer and gyroscope, sampled at the highest possible frequency supported by the devices. Readings from these sensors were recorded while users performed activities like biking, sitting, standing, walking, and so on.\n",
        "\n",
        "Run the cell below to verify you can access the files properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xz3f79crEEb",
        "outputId": "93886259-054b-4014-b48e-2124ae6b43cf"
      },
      "source": [
        "# dont forget to change file path back to original here\n",
        "static = spark.read.json(\"/content/drive/MyDrive/CSCD/cscd 430/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")\n",
        "\n",
        "static.show(2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------------+--------+-----+------+----+-----+-------------+-----------+-------------+\n",
            "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|            x|          y|            z|\n",
            "+-------------+-------------------+--------+-----+------+----+-----+-------------+-----------+-------------+\n",
            "|1424686735175|1424686733176178965|nexus4_1|   35|nexus4|   g|stand| 0.0014038086|  5.0354E-4|-0.0124053955|\n",
            "|1424686735378|1424686733382813486|nexus4_1|   76|nexus4|   g|stand|-0.0039367676|0.026138306|  -0.01133728|\n",
            "+-------------+-------------------+--------+-----+------+----+-----+-------------+-----------+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q1: Query the static DataFrame (4 points)**"
      ],
      "metadata": {
        "id": "MXzUoyShoR5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this query, find the count of each type of activities collected from each device. Sort the result based on the device ID and the activity count."
      ],
      "metadata": {
        "id": "7YYUaZIRqVN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code goes here\n",
        "\n",
        "static.createOrReplaceTempView(\"stat\")\n",
        "\n",
        "q1 = spark.sql(\n",
        "    '''SELECT Device, gt, count(gt) as activity_count\n",
        "       FROM stat\n",
        "       GROUP BY Device, gt\n",
        "       ORDER BY Device, activity_count\n",
        "    ''')\n",
        "\n",
        "q1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4nSdT9aqqke",
        "outputId": "e52caa54-b4a1-43f9-82e6-ada8ed8bf4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------------+\n",
            "|  Device|        gt|activity_count|\n",
            "+--------+----------+--------------+\n",
            "|nexus4_1|stairsdown|          4788|\n",
            "|nexus4_1|      bike|          5013|\n",
            "|nexus4_1|      null|          5181|\n",
            "|nexus4_1|  stairsup|          5278|\n",
            "|nexus4_1|     stand|          5395|\n",
            "|nexus4_1|       sit|          6294|\n",
            "|nexus4_1|      walk|          6705|\n",
            "|nexus4_2|stairsdown|          4577|\n",
            "|nexus4_2|  stairsup|          5174|\n",
            "|nexus4_2|      null|          5268|\n",
            "|nexus4_2|      bike|          5783|\n",
            "|nexus4_2|     stand|          5989|\n",
            "|nexus4_2|       sit|          6015|\n",
            "|nexus4_2|      walk|          6551|\n",
            "+--------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Streaming DataFrame"
      ],
      "metadata": {
        "id": "C0L5K3MFvqWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we want to create a **streaming** DataFrame. By default, Structured Streaming from file-based sources requires you to specify the schema, rather than rely on Spark to infer it automatically. Now let's create a streaming DataFrame using the data schema from the above static DataFrame.\n"
      ],
      "metadata": {
        "id": "XZVTiFr2ni9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataSchema = static.schema\n",
        "\n",
        "# dont forget to change file path back to original\n",
        "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
        "    .json(\"/content/drive/MyDrive/CSCD/cscd 430/activity-data\")"
      ],
      "metadata": {
        "id": "DYfPkWYYnip_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here `maxFilesPerTrigger` allows us to control how quickly Spark will read all of the files in the folder. By specifying this value lower, we’re partificially limiting the flow of the stream to one file per trigger. This helps us demonstrate how Structured Streaming runs incrementally in this exercise. Note that this streaming DataFrame is going to use all the files inside `activity-data` folder.\n"
      ],
      "metadata": {
        "id": "j9b__yKumlAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q2: Query the streaming DataFrame (4 points)**"
      ],
      "metadata": {
        "id": "06ijb0dbwIIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming DataFrames are largely the same as static DataFrames. Basically, all of the transformations that are available in the static Structured APIs apply to Streaming DataFrames."
      ],
      "metadata": {
        "id": "nP0_dOZewHQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this query, you define the exact same query as Q1, **but use the streaming DataFrame instead**."
      ],
      "metadata": {
        "id": "mN4rm4Vwy6sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code goes here\n",
        "\n",
        "treaming.createOrReplaceTempView(\"stream\")\n",
        "\n",
        "q2 = spark.sql(\n",
        "    '''SELECT Device, gt, count(gt) as activity_count\n",
        "       FROM stream\n",
        "       GROUP BY Device, gt\n",
        "       ORDER BY Device, activity_count\n",
        "    ''')\n",
        "\n"
      ],
      "metadata": {
        "id": "2KlSaUdHyeha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set the shuffle partitions to a small value to avoid creating too many shuffle partitions."
      ],
      "metadata": {
        "id": "YNNj0Dw30VAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
      ],
      "metadata": {
        "id": "GqcjHH2Z0cAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q3: Start the streaming query (4 points)**"
      ],
      "metadata": {
        "id": "2QpC5Sgz4wsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you want to start this streaming query. You need to write the output to an in-momory table called `activity_counts` (later you will query this table to see the query results). Please use `complete` as the [output mode](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes) and use `memory` as the [output sink](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks) for this query."
      ],
      "metadata": {
        "id": "xzR_II3s095x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code goes here\n",
        "\n",
        "query = q2.writeStream.outputMode(\"complete\").format(\"memory\").queryName(\"activity_counts\").start()"
      ],
      "metadata": {
        "id": "flJWhfgWYtOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now this stream is running. We can experiment with the results by querying the in-memory table `activity_counts` you have created in the previous step. We’ll do this in a simple loop that will print the results of the streaming query every three seconds. Your results might be different from mine, but overall, you should be able to see the incremental activity counts as the query loops through."
      ],
      "metadata": {
        "id": "xjMYPeeep_bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "for x in range(4):\n",
        "      spark.sql(\"SELECT * FROM activity_counts\").show()\n",
        "      sleep(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGfVdYn7c_oF",
        "outputId": "4c631ef9-5a2e-4a13-eb70-577067108be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+--------------+\n",
            "|Device| gt|activity_count|\n",
            "+------+---+--------------+\n",
            "+------+---+--------------+\n",
            "\n",
            "+--------+----------+--------------+\n",
            "|  Device|        gt|activity_count|\n",
            "+--------+----------+--------------+\n",
            "|nexus4_1|stairsdown|          4815|\n",
            "|nexus4_1|      bike|          4892|\n",
            "|nexus4_1|      null|          5247|\n",
            "|nexus4_1|  stairsup|          5273|\n",
            "|nexus4_1|     stand|          5357|\n",
            "|nexus4_1|       sit|          6217|\n",
            "|nexus4_1|      walk|          6665|\n",
            "|nexus4_2|stairsdown|          4547|\n",
            "|nexus4_2|  stairsup|          5183|\n",
            "|nexus4_2|      null|          5200|\n",
            "|nexus4_2|      bike|          5905|\n",
            "|nexus4_2|     stand|          6030|\n",
            "|nexus4_2|       sit|          6090|\n",
            "|nexus4_2|      walk|          6591|\n",
            "+--------+----------+--------------+\n",
            "\n",
            "+--------+----------+--------------+\n",
            "|  Device|        gt|activity_count|\n",
            "+--------+----------+--------------+\n",
            "|nexus4_1|stairsdown|          9526|\n",
            "|nexus4_1|      bike|          9806|\n",
            "|nexus4_1|      null|         10538|\n",
            "|nexus4_1|  stairsup|         10566|\n",
            "|nexus4_1|     stand|         10924|\n",
            "|nexus4_1|       sit|         12390|\n",
            "|nexus4_1|      walk|         13409|\n",
            "|nexus4_2|stairsdown|          9197|\n",
            "|nexus4_2|  stairsup|         10347|\n",
            "|nexus4_2|      null|         10357|\n",
            "|nexus4_2|      bike|         11789|\n",
            "|nexus4_2|     stand|         11849|\n",
            "|nexus4_2|       sit|         12224|\n",
            "|nexus4_2|      walk|         13102|\n",
            "+--------+----------+--------------+\n",
            "\n",
            "+--------+----------+--------------+\n",
            "|  Device|        gt|activity_count|\n",
            "+--------+----------+--------------+\n",
            "|nexus4_1|stairsdown|         14219|\n",
            "|nexus4_1|      bike|         14815|\n",
            "|nexus4_1|      null|         15709|\n",
            "|nexus4_1|  stairsup|         15868|\n",
            "|nexus4_1|     stand|         16595|\n",
            "|nexus4_1|       sit|         18797|\n",
            "|nexus4_1|      walk|         19861|\n",
            "|nexus4_2|stairsdown|         13871|\n",
            "|nexus4_2|  stairsup|         15495|\n",
            "|nexus4_2|      null|         15635|\n",
            "|nexus4_2|     stand|         17563|\n",
            "|nexus4_2|      bike|         17577|\n",
            "|nexus4_2|       sit|         18125|\n",
            "|nexus4_2|      walk|         19906|\n",
            "+--------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understand Event Time"
      ],
      "metadata": {
        "id": "OyyIBv7N5JFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In stream processing systems, there are two relevant times for each event: the time at which it actually occurred (***event time***), and the time that it was processed or reached the stream processing system (***processing time***).\n",
        "\n",
        "In the activity dataset, there are two time-based columns. The `Creation_Time` column defines when an event was created, whereas the `Arrival_Time` defines when an event hit the servers. We will use `Creation_Time` for our experiments.\n",
        "\n",
        "The first step in event-time analysis is to convert the timestamp column into the proper Spark SQL timestamp type. The current `Creation_Time` column is unixtime nanoseconds (represented as a long), therefore we need to do a little manipulation to get it into the proper timestamp format."
      ],
      "metadata": {
        "id": "IygvFt79GHf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "withEventTime = streaming.selectExpr(\"*\", \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
      ],
      "metadata": {
        "id": "iZC49Bop_qzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that a new column `event_time` is added to the streaming DataFrame `withEventTime`:"
      ],
      "metadata": {
        "id": "CPbR3zEvHyyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "withEventTime.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjqVSnP__3ip",
        "outputId": "27cd8860-c090-4dea-c7b6-39813bd6bf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Arrival_Time: long (nullable = true)\n",
            " |-- Creation_Time: long (nullable = true)\n",
            " |-- Device: string (nullable = true)\n",
            " |-- Index: long (nullable = true)\n",
            " |-- Model: string (nullable = true)\n",
            " |-- User: string (nullable = true)\n",
            " |-- gt: string (nullable = true)\n",
            " |-- x: double (nullable = true)\n",
            " |-- y: double (nullable = true)\n",
            " |-- z: double (nullable = true)\n",
            " |-- event_time: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q4: Query Tumbling Windows (4 points)**"
      ],
      "metadata": {
        "id": "XdR8hpOgIX9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark has two types of [Windows](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#types-of-time-windows) - Tumbling Window and Sliding Window. The main difference between these windows is that, tumbling windows are **non-overlapping** whereas sliding windows can be **overlapping**."
      ],
      "metadata": {
        "id": "qmmapx5lJPzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first query non-overlapping **tumbling windows**. The query is to count the number of activities in **each window of 20 minutes** based on the `event_time` in the `withEventTime` DataFrame. We want to show the streaming query results incrementally by setting the output mode to `complete` and sending the output to an in-memory table called `tumbling_windows`. Please sort the query results based on the time ordering of these windows."
      ],
      "metadata": {
        "id": "xyZ6WkPEJyQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import window, col\n",
        "\n",
        "# your code goes here\n",
        "\n"
      ],
      "metadata": {
        "id": "Wzrvj6oBCw9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's query the in-memory table `tumbling_windows` 4 times to see the incremental results."
      ],
      "metadata": {
        "id": "pOl_Hxf2MpMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "for x in range(4):\n",
        "      spark.sql(\"SELECT * FROM tumbling_windows\").show(truncate=False)\n",
        "      sleep(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9k-RDogC142",
        "outputId": "bf1d485b-0ab4-4a0c-f7cb-01c86aca68cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|window|count|\n",
            "+------+-----+\n",
            "+------+-----+\n",
            "\n",
            "+------+-----+\n",
            "|window|count|\n",
            "+------+-----+\n",
            "+------+-----+\n",
            "\n",
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2015-02-22 00:40:00, 2015-02-22 01:00:00}|1    |\n",
            "|{2015-02-23 10:00:00, 2015-02-23 10:20:00}|115  |\n",
            "|{2015-02-23 10:20:00, 2015-02-23 10:40:00}|2493 |\n",
            "|{2015-02-23 10:40:00, 2015-02-23 11:00:00}|3211 |\n",
            "|{2015-02-23 11:00:00, 2015-02-23 11:20:00}|2451 |\n",
            "|{2015-02-23 11:20:00, 2015-02-23 11:40:00}|898  |\n",
            "|{2015-02-23 12:00:00, 2015-02-23 12:20:00}|708  |\n",
            "|{2015-02-23 12:20:00, 2015-02-23 12:40:00}|2566 |\n",
            "|{2015-02-23 12:40:00, 2015-02-23 13:00:00}|2545 |\n",
            "|{2015-02-23 13:00:00, 2015-02-23 13:20:00}|4172 |\n",
            "|{2015-02-23 13:20:00, 2015-02-23 13:40:00}|2457 |\n",
            "|{2015-02-23 13:40:00, 2015-02-23 14:00:00}|4511 |\n",
            "|{2015-02-23 14:00:00, 2015-02-23 14:20:00}|3364 |\n",
            "|{2015-02-23 14:20:00, 2015-02-23 14:40:00}|2269 |\n",
            "|{2015-02-23 14:40:00, 2015-02-23 15:00:00}|940  |\n",
            "|{2015-02-24 11:00:00, 2015-02-24 11:20:00}|988  |\n",
            "|{2015-02-24 11:20:00, 2015-02-24 11:40:00}|2580 |\n",
            "|{2015-02-24 11:40:00, 2015-02-24 12:00:00}|3708 |\n",
            "|{2015-02-24 12:00:00, 2015-02-24 12:20:00}|5118 |\n",
            "|{2015-02-24 12:20:00, 2015-02-24 12:40:00}|3176 |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2015-02-22 00:40:00, 2015-02-22 01:00:00}|1    |\n",
            "|{2015-02-23 10:00:00, 2015-02-23 10:20:00}|282  |\n",
            "|{2015-02-23 10:20:00, 2015-02-23 10:40:00}|5033 |\n",
            "|{2015-02-23 10:40:00, 2015-02-23 11:00:00}|6315 |\n",
            "|{2015-02-23 11:00:00, 2015-02-23 11:20:00}|4845 |\n",
            "|{2015-02-23 11:20:00, 2015-02-23 11:40:00}|1861 |\n",
            "|{2015-02-23 12:00:00, 2015-02-23 12:20:00}|1478 |\n",
            "|{2015-02-23 12:20:00, 2015-02-23 12:40:00}|5164 |\n",
            "|{2015-02-23 12:40:00, 2015-02-23 13:00:00}|5047 |\n",
            "|{2015-02-23 13:00:00, 2015-02-23 13:20:00}|8227 |\n",
            "|{2015-02-23 13:20:00, 2015-02-23 13:40:00}|4974 |\n",
            "|{2015-02-23 13:40:00, 2015-02-23 14:00:00}|9113 |\n",
            "|{2015-02-23 14:00:00, 2015-02-23 14:20:00}|6492 |\n",
            "|{2015-02-23 14:20:00, 2015-02-23 14:40:00}|4695 |\n",
            "|{2015-02-23 14:40:00, 2015-02-23 15:00:00}|1874 |\n",
            "|{2015-02-24 11:00:00, 2015-02-24 11:20:00}|2022 |\n",
            "|{2015-02-24 11:20:00, 2015-02-24 11:40:00}|5133 |\n",
            "|{2015-02-24 11:40:00, 2015-02-24 12:00:00}|7515 |\n",
            "|{2015-02-24 12:00:00, 2015-02-24 12:20:00}|10021|\n",
            "|{2015-02-24 12:20:00, 2015-02-24 12:40:00}|6484 |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q5: Query Sliding Windows (4 points)**"
      ],
      "metadata": {
        "id": "4JjtnwJ7RQpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we want to develop a similar query to the streaming DataFrame `withEventTime`, but this time we want to define our query using **sliding windows**. We want to use the same **window width of 20 minutes**, but we want to slide the window through the timeline **every 10 minutes**. This time send the query results to an in-memory table called `sliding_windows`. Please make sure to sort the query results based on the time ordering of these windows."
      ],
      "metadata": {
        "id": "j0vOfIshTDo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import window, col\n",
        "\n",
        "# your code goes here"
      ],
      "metadata": {
        "id": "qhC_haE9U2DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's query the in-memory table `sliding_windows` 4 times to see the incremental results."
      ],
      "metadata": {
        "id": "y3VJIdCMWFMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "for x in range(4):\n",
        "      spark.sql(\"SELECT * FROM sliding_windows\").show(truncate=False)\n",
        "      sleep(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt7wlZ1jUxIQ",
        "outputId": "5675b746-a7ce-47c3-c929-5a5928b9219f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|window|count|\n",
            "+------+-----+\n",
            "+------+-----+\n",
            "\n",
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2015-02-22 00:30:00, 2015-02-22 00:50:00}|1    |\n",
            "|{2015-02-22 00:40:00, 2015-02-22 01:00:00}|1    |\n",
            "|{2015-02-23 10:00:00, 2015-02-23 10:20:00}|115  |\n",
            "|{2015-02-23 10:10:00, 2015-02-23 10:30:00}|1316 |\n",
            "|{2015-02-23 10:20:00, 2015-02-23 10:40:00}|2493 |\n",
            "|{2015-02-23 10:30:00, 2015-02-23 10:50:00}|2404 |\n",
            "|{2015-02-23 10:40:00, 2015-02-23 11:00:00}|3211 |\n",
            "|{2015-02-23 10:50:00, 2015-02-23 11:10:00}|3406 |\n",
            "|{2015-02-23 11:00:00, 2015-02-23 11:20:00}|2451 |\n",
            "|{2015-02-23 11:10:00, 2015-02-23 11:30:00}|2042 |\n",
            "|{2015-02-23 11:20:00, 2015-02-23 11:40:00}|898  |\n",
            "|{2015-02-23 12:00:00, 2015-02-23 12:20:00}|708  |\n",
            "|{2015-02-23 12:10:00, 2015-02-23 12:30:00}|2067 |\n",
            "|{2015-02-23 12:20:00, 2015-02-23 12:40:00}|2566 |\n",
            "|{2015-02-23 12:30:00, 2015-02-23 12:50:00}|2458 |\n",
            "|{2015-02-23 12:40:00, 2015-02-23 13:00:00}|2545 |\n",
            "|{2015-02-23 12:50:00, 2015-02-23 13:10:00}|3475 |\n",
            "|{2015-02-23 13:00:00, 2015-02-23 13:20:00}|4172 |\n",
            "|{2015-02-23 13:10:00, 2015-02-23 13:30:00}|3297 |\n",
            "|{2015-02-23 13:20:00, 2015-02-23 13:40:00}|2457 |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2015-02-22 00:30:00, 2015-02-22 00:50:00}|1    |\n",
            "|{2015-02-22 00:40:00, 2015-02-22 01:00:00}|1    |\n",
            "|{2015-02-23 10:00:00, 2015-02-23 10:20:00}|115  |\n",
            "|{2015-02-23 10:10:00, 2015-02-23 10:30:00}|1316 |\n",
            "|{2015-02-23 10:20:00, 2015-02-23 10:40:00}|2493 |\n",
            "|{2015-02-23 10:30:00, 2015-02-23 10:50:00}|2404 |\n",
            "|{2015-02-23 10:40:00, 2015-02-23 11:00:00}|3211 |\n",
            "|{2015-02-23 10:50:00, 2015-02-23 11:10:00}|3406 |\n",
            "|{2015-02-23 11:00:00, 2015-02-23 11:20:00}|2451 |\n",
            "|{2015-02-23 11:10:00, 2015-02-23 11:30:00}|2042 |\n",
            "|{2015-02-23 11:20:00, 2015-02-23 11:40:00}|898  |\n",
            "|{2015-02-23 12:00:00, 2015-02-23 12:20:00}|708  |\n",
            "|{2015-02-23 12:10:00, 2015-02-23 12:30:00}|2067 |\n",
            "|{2015-02-23 12:20:00, 2015-02-23 12:40:00}|2566 |\n",
            "|{2015-02-23 12:30:00, 2015-02-23 12:50:00}|2458 |\n",
            "|{2015-02-23 12:40:00, 2015-02-23 13:00:00}|2545 |\n",
            "|{2015-02-23 12:50:00, 2015-02-23 13:10:00}|3475 |\n",
            "|{2015-02-23 13:00:00, 2015-02-23 13:20:00}|4172 |\n",
            "|{2015-02-23 13:10:00, 2015-02-23 13:30:00}|3297 |\n",
            "|{2015-02-23 13:20:00, 2015-02-23 13:40:00}|2457 |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2015-02-22 00:30:00, 2015-02-22 00:50:00}|1    |\n",
            "|{2015-02-22 00:40:00, 2015-02-22 01:00:00}|1    |\n",
            "|{2015-02-23 10:00:00, 2015-02-23 10:20:00}|282  |\n",
            "|{2015-02-23 10:10:00, 2015-02-23 10:30:00}|2756 |\n",
            "|{2015-02-23 10:20:00, 2015-02-23 10:40:00}|5033 |\n",
            "|{2015-02-23 10:30:00, 2015-02-23 10:50:00}|4802 |\n",
            "|{2015-02-23 10:40:00, 2015-02-23 11:00:00}|6315 |\n",
            "|{2015-02-23 10:50:00, 2015-02-23 11:10:00}|6650 |\n",
            "|{2015-02-23 11:00:00, 2015-02-23 11:20:00}|4845 |\n",
            "|{2015-02-23 11:10:00, 2015-02-23 11:30:00}|4128 |\n",
            "|{2015-02-23 11:20:00, 2015-02-23 11:40:00}|1861 |\n",
            "|{2015-02-23 12:00:00, 2015-02-23 12:20:00}|1478 |\n",
            "|{2015-02-23 12:10:00, 2015-02-23 12:30:00}|4170 |\n",
            "|{2015-02-23 12:20:00, 2015-02-23 12:40:00}|5164 |\n",
            "|{2015-02-23 12:30:00, 2015-02-23 12:50:00}|4898 |\n",
            "|{2015-02-23 12:40:00, 2015-02-23 13:00:00}|5047 |\n",
            "|{2015-02-23 12:50:00, 2015-02-23 13:10:00}|6841 |\n",
            "|{2015-02-23 13:00:00, 2015-02-23 13:20:00}|8227 |\n",
            "|{2015-02-23 13:10:00, 2015-02-23 13:30:00}|6619 |\n",
            "|{2015-02-23 13:20:00, 2015-02-23 13:40:00}|4974 |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on the completion of another colab assignment! I hope you learned the basic concepts related to Spark Structured Streaming through this assignment. I would recommend you to read the rest of [this document](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#structured-streaming-programming-guide) and further explore other features such as [Handling Late Data and Watermarking](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking), [Join Operations](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#join-operations), and [Streaming Deduplication](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication), etc. Thank you!"
      ],
      "metadata": {
        "id": "AC2utViUW-8w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRT2geHpJZPx"
      },
      "source": [
        "##Reference\n",
        "*Spark: The Definitive Guide* (2018), by Bill Chambers and Matei Zaharia, ISBN: 9781491912218"
      ]
    }
  ]
}