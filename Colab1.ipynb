{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Colab 1: Spark and Spark RDD (10 points)\n",
        "\n",
        "In this lab you will learn how to use Apache Spark on a Colab enviroment and learn the concepts of Spark RDD."
      ],
      "metadata": {
        "id": "0AbLCnzu7e-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set Up\n",
        "Let's setup Spark on your Colab environment. Run the cell below."
      ],
      "metadata": {
        "id": "DCUJqDtH8YSg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqFdmQj1GH_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4395b36c-f337-4c5c-af87-cf328b5986e8"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=1d59e2eb3e14047bafea5e8cfc541c3b31488958c6a3a8080594af5d72c27d33\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121654 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u392-ga-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u392-ga-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u392-ga-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, import libraries and create a new Spark session."
      ],
      "metadata": {
        "id": "g-TfrtNw8ts3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zthefrOOGx2x"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark import SparkContext, SparkConf\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "ZUojdlQjBk3q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RDD Programming\n",
        "\n",
        "Please follow [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html) to learn RDD related concepts and answer the following 8 questions in this lab."
      ],
      "metadata": {
        "id": "dN-yIApQ9W7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q1: What does RDD stand for? What is a RDD in Spark? (1 point)\n",
        "\n",
        "RDD stands for Resilient Distributed Dataset. In Spark, an RDD is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel, i.e. its a data strucutre that allows for parallel computations across a cluster."
      ],
      "metadata": {
        "id": "XdgpKwNH-2OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q2: Create a new RDD using `parallelize()` (1 point)"
      ],
      "metadata": {
        "id": "550Dw_YhBt0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create RDDs from Python collections (lists) using `parallelize()` method. Now create your first RDD named `rdd1`."
      ],
      "metadata": {
        "id": "cf-if25OCUJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4, 5]\n",
        "# Create rdd1 from data\n",
        "# your code goes here...\n",
        "\n",
        "rdd1 = sc.parallelize(data)"
      ],
      "metadata": {
        "id": "3m8_Su7EAasp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return all the elements in rdd1\n",
        "rdd1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFNhvgVeEFdp",
        "outputId": "7a71107f-f771-4916-e889-0990ab87dfc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q3: Create a new RDD using `textFile()` (1 point)\n"
      ],
      "metadata": {
        "id": "ffVpHOugHjb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create RDDs from local files using `textFile()` method. Now upload the file \"data.txt\" to your current work space, then create your second RDD named `rdd2` from this file."
      ],
      "metadata": {
        "id": "op5NMA39H5_u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhS7gaW3G_u0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55314be7-4152-4242-f769-5694de73bd70"
      },
      "source": [
        "# Create rdd2 from local file \"data.txt\"\n",
        "# your code goes here...\n",
        "\n",
        "# Afterwards, return the first 10 elements of rdd2.\n",
        "# your code goes here...\n",
        "\n",
        "rdd2 = sc.textFile(\"/content/data.txt\")\n",
        "rdd2.take(10)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1500',\n",
              " '12419',\n",
              " '746316',\n",
              " '1 2 1',\n",
              " '1 39 1',\n",
              " '1 42 3',\n",
              " '1 77 1',\n",
              " '1 95 1',\n",
              " '1 96 1',\n",
              " '1 105 1']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q4: RDD Operations (3 points)"
      ],
      "metadata": {
        "id": "zPy-giPeJOQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDDs support two types of operations, *transformations* and *actions*. Answer the following questions:\n",
        "\n",
        "1.   What is a transformation operation? Provide two examples of transformation operations.\n",
        "2.   What is an action operation? Provide two examples of action operations.\n",
        "3.   What does lazy evaluation mean? What is the benefit of lazy evaluation?\n"
      ],
      "metadata": {
        "id": "rNaWk5t9Kge9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A transformation operation is an operation that takes an existing RDD and returns a new RDD from the given one. These are not necessarily executed right away, as they are added to a DAG and executed when the driver says to. One example of a transformation operation is union(). It returns a new dataset that contains the union of the elements in the source dataset and the argument. Another example is distinct(), which returns a new RDD that contains the distinct elements of the source dataset.\n",
        "\n",
        "2. An action operation is an operation that does not return an RDD. One example of an action is reduce(), which aggregates the elements of the dataset using a user defined function. Another example is count(), which returns the number of elements of the dataset.\n",
        "\n",
        "3. Lazy evaluation in Spark means that when we call an action or transformation, it will not necessarily be executed right away. These transformations will be added to a DAG that is used to maintain the order in which the transformations should be applied to RDDs. Spark defers the execution of the action or transformation until needed, which leads to better optimization and performance, saving time and unwanted processing power. Essentially, lazy evaluation is doing nothing until you actually require the final result to be computed.  "
      ],
      "metadata": {
        "id": "aAL4UYKyKgQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Review: Python `lambda` Functions\n",
        "\n",
        "\n",
        "*   Small anonymous functions not bound to a name\n",
        "*   Example: `lambda a, b: a + b`, returns the sum of its two arguments\n",
        "*   Can use lambda functions wherever function objects are required\n",
        "*   Restricted to a single expression"
      ],
      "metadata": {
        "id": "YfYhgXNjMbg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q5: RDD `map()` function (1 point)"
      ],
      "metadata": {
        "id": "2_3-L2qVNZ-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Q2, you have created `rdd1`. Now use RDD `map()` function to double the values of elements in `rdd1`, then display the result."
      ],
      "metadata": {
        "id": "twsy_-k1Q3e7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvo6cGTeI-u7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4428f935-c4f1-45f6-863a-e8d181633d7e"
      },
      "source": [
        "# your code goes here...\n",
        "\n",
        "rdd1.map(lambda a: 2*a).collect()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q6: RDD `filter()` function (1 point)"
      ],
      "metadata": {
        "id": "BPAj0YIuUAlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use RDD `filter()` function to return only those values that are divisible by 4 from the above result."
      ],
      "metadata": {
        "id": "RO_41QOiaeFF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctASn51pLMgN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac025a5c-f980-4843-c87c-9f6304c57d46"
      },
      "source": [
        "# your code goes here...\n",
        "\n",
        "rdd1.map(lambda a: 2*a).filter(lambda x: x % 4 == 0).collect()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q7: RDD `reduce()` function (1 point)"
      ],
      "metadata": {
        "id": "VPY2ab09eLMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use RDD `reduce()` function to calculate the product of all the values in `rdd1`. Display the output."
      ],
      "metadata": {
        "id": "bEMMxU3fdvUM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX3_6v20HNGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b64ef92-f6ac-486b-ef2e-1ad7aabf88df"
      },
      "source": [
        "# your code goes here...\n",
        "from operator import mul\n",
        "rdd1.reduce(mul)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8: RDD `reduceByKey()` function (1 point)"
      ],
      "metadata": {
        "id": "4jVO0MfWfEVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark supports Key-Value pairs. In the cell below we create a RDD named `rdd3` which includes 5 key-value pairs. Let's assume the keys are the words in documents, and the values are the frequencies of the corresponding words in documents."
      ],
      "metadata": {
        "id": "sco0IRFmfOjw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z80ZwNGkLZ4q"
      },
      "source": [
        "rdd3 = sc.parallelize([(\"hello\",2), (\"world\",4), (\"hello\",10), (\"world\", 3), (\"!\", 100)])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you use `reduceByKey()` function to aggregate all the frequencies of the same word, and disply the result after aggregation."
      ],
      "metadata": {
        "id": "qks0KGLihLDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code goes here...\n",
        "from operator import add\n",
        "rdd3.reduceByKey(add).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxrQriMQhF0b",
        "outputId": "43ded186-8688-4d76-c3e6-5d547ef3932c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('world', 7), ('!', 100), ('hello', 12)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Congratulations on the completion of your first lab assignment!"
      ],
      "metadata": {
        "id": "ytWRxzkqm8cG"
      }
    }
  ]
}